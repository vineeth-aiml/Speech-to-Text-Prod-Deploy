

# Offline Speech-to-Text + MoM  — V1 / V2 / V3 (CPU) + Best UI + FastAPI + Docker

A **CPU-only, offline-first** meeting assistant that provides:

- **Live streaming Speech-to-Text (STT)** via **WebSocket**
- **Multilingual**: English + Indian languages (Whisper multilingual)
- **MoM (Minutes of Meeting)** in the **same language as the meeting**
- **V1 / V2 / V3** maturity levels (product-company style)
- **Best UI** (React dashboard)
- **FastAPI backend**
- **Docker deployment** (one-command run)

---

## Why this project?

### Problem Statement
In real meetings (team syncs, client calls, standups), people face:

- Long meetings → decisions and action items get lost
- Manual MoM writing wastes time and misses details
- Many orgs require **offline** solutions for privacy/compliance
- Multi-lingual meetings (English + Hindi/Telugu/etc.) are common
- Need **live transcript** + **structured MoM** + **exports** for sharing

This repo solves that with an **offline-first**, **CPU-only** approach.

---

## Use Cases

### Engineering / Product
- Standups, sprint planning, incident postmortems
- Live transcript on screen
- MoM with decisions + action items

### Client / Sales / Support
- Capture requirements and commitments
- Generate MoM in same language as speech
- Export `.txt` / `.md` and share

### Education / Interviews
- Lecture transcript + summary
- Multi-language support
- Works offline

---

## V1 / V2 / V3 — What each version means

### V1 — Baseline (Always works offline)
✅ Live streaming STT (partial updates)  
✅ Save meeting transcript  
✅ Rule-based MoM (offline)

### V2 — Better MoM (Offline LLM)
✅ MoM generated by **local LLM** via **Ollama** (best quality offline)  
✅ Auto fallback to V1 if Ollama not running  
✅ MoM output language = transcript language

### V3 — Product / Team-ready (Enterprise direction)
✅ **Room-based collaboration** (multi-tab/users) using `room_id`  
✅ **Exports** `.txt` and `.md`  
✅ **Feature flags** for safe rollout (FAANG style)  
✅ Eval hooks placeholders (WER/latency) for CI gating

---

## Architecture

### High-level flow
1. Browser microphone audio → small PCM chunks → WebSocket `/ws/stt`
2. Backend assembles `.wav` buffer
3. Whisper (CPU) transcribes periodically → sends **partial transcript** updates
4. Transcript saved as a meeting record (JSON)
5. MoM generated:
   - **V2**: Ollama local LLM
   - **Fallback**: V1 rule-based
6. Export as `.txt` / `.md`

### Components
- **Frontend (React + Vite)**  
  Live transcript, meeting list, MoM view, export buttons
- **Backend (FastAPI + WebSocket)**  
  Streaming STT, MoM engine, storage, exports
- **Ops**  
  Feature flags, Docker setup, eval hooks

---

## Tech Stack

### Backend
- Python 3.10+
- FastAPI (REST + WebSocket)
- faster-whisper (CPU multilingual STT)
- requests (Ollama API)
- PyYAML (feature flags)

### Frontend
- React 18
- Vite
- WebAudio API (mic capture)
- WebSocket client

### Optional (V2)
- Ollama (offline local LLM)
  - Recommended CPU models:
    - `qwen2.5:3b-instruct`
    - `phi3:mini`
    - `llama3.2:3b-instruct`

---

## Project Structure

```
.
├── docker-compose.yml
├── README.md
├── ops/
│   └── feature_flags.yaml
├── prompts/
│   └── mom_v2_prompt.txt
├── models/
│   └── whisper/              # optional offline models
├── data/
│   └── meetings/             # saved meeting JSON files
├── backend/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── app/
│       ├── main.py
│       ├── ws_stt.py
│       ├── stt_engine.py
│       ├── rooms.py
│       ├── storage.py
│       ├── exports.py
│       ├── mom_engine.py
│       ├── mom_engine_v1.py
│       └── mom_engine_v2_ollama.py
└── frontend/
    ├── Dockerfile
    ├── nginx.conf
    ├── package.json
    ├── vite.config.js
    ├── index.html
    └── src/
        ├── main.jsx
        ├── App.jsx
        ├── styles/
        │   └── app.css
        └── components/
            ├── MeetingList.jsx
            └── MoMView.jsx
```

---

## Quick Start (Docker)

### 1) Start backend + UI
```bash
docker compose up --build
```

- Frontend UI: http://localhost:5173  
- Backend Swagger: http://localhost:8000/docs  

### 2) (Optional) Enable best MoM with Ollama (V2)
```bash
ollama run qwen2.5:3b-instruct
```

If Ollama is not running, MoM automatically falls back to V1.

---

## Run Without Docker (Windows CPU)

### Backend
```bash
cd backend
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

### Frontend
```bash
cd frontend
npm install
npm run dev
```

Open: http://localhost:5173

---

## Configuration (Feature Flags)

Edit `ops/feature_flags.yaml` and restart backend.

Common flags:
- `MOM_ENGINE`: `"v1"` or `"v2"` (default `"v2"`)
- `WHISPER_MODEL`: `tiny | base | small`
- `LANG_DEFAULT`: `auto | en | hi | te | ta | kn | ml | mr | bn`
- `PARTIAL_EVERY_N_CHUNKS`: partial update frequency
- `ROOM_BROADCAST`: true/false (V3 rooms)
- `EXPORTS_ENABLED`: true/false (V3 exports)
- `OLLAMA_MODEL`: e.g. `qwen2.5:3b-instruct`

**CPU Tips**
- For faster STT: set `WHISPER_MODEL: base`
- For higher accuracy: `WHISPER_MODEL: small` (slower)
- For faster MoM: use smaller Ollama models

---

## Inputs & Outputs Examples (Long Meeting)

### Input (UI)
- Room ID: `team-sync`
- Language: `auto`
- Click **Start** and speak/play meeting audio for 30–60 mins
- Click **Generate MoM**
- Click **Export .txt / .md**

### Input (MoM API) — internal payload
```json
{
  "meeting_id": "1700000000000",
  "transcript": "…(very long transcript)…",
  "language": "auto"
}
```

### Output: Transcript (shortened)
```
Today we will finalize Q1 roadmap. Payment retries caused duplicate charges.
We need to fix idempotency at checkout. Rahul will deliver patch by Wednesday.
We will improve p95 latency alerts. Priya will add dashboards.
We decided to open one backend role and one data role.
Please share the PRD by tomorrow evening.
```

### Output: MoM JSON (V2 example)
```json
{
  "summary": [
    "Reviewed Q1 roadmap and prioritized reliability improvements.",
    "Checkout idempotency and latency monitoring were identified as top actions.",
    "Hiring plan and PRD deadline were confirmed."
  ],
  "decisions": [
    "Deploy idempotency fix to staging by Wednesday.",
    "Add p95 latency SLO dashboards and alerts before production rollout.",
    "Open hiring for one backend role and one data role."
  ],
  "action_items": [
    { "task": "Implement checkout idempotency patch and deploy to staging", "owner": "Rahul", "due_date": "Wednesday" },
    { "task": "Create p95 latency dashboard and SLO alerts", "owner": "Priya", "due_date": "" },
    { "task": "Share final PRD with stakeholders", "owner": "", "due_date": "tomorrow evening" }
  ],
  "risks": [
    "Delay in idempotency fix may lead to continued duplicate charges.",
    "Insufficient monitoring may hide latency regressions during rollout."
  ],
  "next_steps": [
    "Validate in staging then do canary rollout in production.",
    "Review monitoring dashboards in next sync.",
    "Finalize PRD and align release timeline."
  ],
  "engine": "v2-ollama",
  "language": "en"
}
```

---

## V3 Collaboration (Rooms)

**Goal:** One person speaks, multiple people view transcript.

### How to test
1. Open UI in **two tabs**
2. Set **same Room ID** in both: `team-sync`
3. Click **Start** in one tab
4. Transcript broadcasts to all tabs automatically (V3)

---

## Exports (V3)
- Export `.txt`: transcript + MoM sections
- Export `.md`: markdown meeting notes

UI buttons call:
- `GET /api/meetings/{meeting_id}/export.txt`
- `GET /api/meetings/{meeting_id}/export.md`

---

## API Endpoints

### WebSocket
- `ws://localhost:8000/ws/stt?room_id=<id>&lang=auto`

### Meetings
- `GET  /api/meetings`
- `GET  /api/meetings/{meeting_id}`
- `POST /api/meetings/save`

### MoM
- `POST /api/mom`

### Exports
- `GET /api/meetings/{meeting_id}/export.txt`
- `GET /api/meetings/{meeting_id}/export.md`

---

## Performance Notes (CPU)
- Whisper STT is CPU-heavy for long audio.
- If partial updates feel slow:
  - set `WHISPER_MODEL: base`
  - increase `PARTIAL_EVERY_N_CHUNKS`
- V2 MoM depends on Ollama model size:
  - use `qwen2.5:3b-instruct` or `phi3:mini` for CPU

---

## Privacy
- Offline-first: transcripts + MoM stored locally in `data/meetings`
- No cloud required (except initial model download if you choose)
- For fully offline Whisper, place models under `models/whisper/`

---

## Limitations
- Speaker diarization (who spoke what) not included yet
- Streaming method uses simple wav-append approach (stable, not ultra-low latency)

---
